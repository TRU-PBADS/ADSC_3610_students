{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d894202-16a1-4ad4-a057-b2f05282c3e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Lecture 11: Spark Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b62e6e7-43a2-46aa-a9f3-123485859825",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this lecture, students should be able to:\n",
    "\n",
    "- Understand the concept of Resilient Distributed Datasets (RDDs) in Apache Spark.\n",
    "- Explain the immutability and fault tolerance features of RDDs.\n",
    "- Describe how RDDs enable parallel processing in a distributed computing environment.\n",
    "- Identify the key features of RDDs, including lazy evaluation and partitioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f419c22-7442-4cf3-8dd3-9c666b34a088",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Introducing Spark DataFrames\n",
    "\n",
    "Like an RDD, a DataFrame is an immutable distributed collection of data. Unlike an RDD, data is organized into named columns, like a table in a relational database. Designed to make large data sets processing even easier\n",
    "\n",
    "Spark DataFrames are a distributed collection of data organized into named columns, similar to a table in a relational database. They provide a higher-level abstraction than RDDs, making it easier to work with structured and semi-structured data. DataFrames support a wide range of operations, including filtering, aggregation, and joining, and they are optimized for performance through the Catalyst query optimizer. This makes them a powerful tool for big data processing and analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d233d1d1-13bf-4fea-8357-020a892840ef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "What makes a Spark DataFrame different from other dataframes such as `pandas` DataFrame is the **distributed** aspect of it, similar to the RDDs concept that we learned in the last lecture. \n",
    "\n",
    "Suppose you have the following table stored in a Spark DataFrame:\n",
    "\n",
    "| ID | Name    | Age | City       |\n",
    "|----|---------|-----|------------|\n",
    "| 1  | Alice   | 30  | New York   |\n",
    "| 2  | Bob     | 25  | Los Angeles|\n",
    "| 3  | Charlie | 35  | Chicago    |\n",
    "| 4  | David   | 40  | Houston    |\n",
    "\n",
    "As a programmer, you will see, manage, and transform this table as if it was a single and unified table. However, under the hoods, Spark splits the data into multiple partitions across clusters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61d0d892-15bb-46cb-a884-7276523d5d3b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![](https://pedropark99.github.io/Introd-pyspark/Figures/partitions-df.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ca133c3-0fc2-4a71-be9e-89502af8a8b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For the most part, you don't manipulate these partitions manually or individually but instead rely on Spark's built-in operations to handle the distribution and parallelism for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7be85922-7fcc-4ce3-89f9-ab5de438a5a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create a `DataFrame` in `pyspark`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf4b2b1c-ecb4-4bcc-80f7-d72f1f6ca336",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's create a sample spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "383b015a-7234-4fd0-9d96-bf5a2074f045",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>Age</th></tr></thead><tbody><tr><td>Alice</td><td>34</td></tr><tr><td>Bob</td><td>45</td></tr><tr><td>Cathy</td><td>29</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Alice",
         34
        ],
        [
         "Bob",
         45
        ],
        [
         "Cathy",
         29
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Age",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "661fe115-1a1e-4cea-850e-b86af00c7e21",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Remember that a Spark DataFrame in python is a object of class pyspark.sql.dataframe.DataFrame as you can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92ce3b1a-77e0-4b89-8303-a1fa41b500f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55965e09-2086-4c9f-8bc7-21405f1d0406",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's try to see what's inside of `df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bab53af6-491c-4604-9bdd-1670a799eb70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: bigint]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c62e00e8-c322-4599-ab6f-f459d397d64a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "When we call for an object that stores a Spark DataFrame, Spark will only calculate and print a summary of the structure of your Spark DataFrame, and not the DataFrame itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a114d73-6776-4a25-9159-1ba86570a616",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To actually see the Spark DataFrame, you need to use the `show()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6a70a3f-a0aa-4cff-a420-0a4d1e6287e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n| Name|Age|\n+-----+---+\n|Alice| 34|\n|  Bob| 45|\n|Cathy| 29|\n+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44b2e09b-afbd-44d7-bc02-d3edee5e21dd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You can also show top `n` rows by using `show(n)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "207b79df-debe-41a1-a0c7-d97356e3ed3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n| Name|Age|\n+-----+---+\n|Alice| 34|\n|  Bob| 45|\n+-----+---+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbc3fe3f-aa7c-4420-a8b1-9fedb82657d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You could also display top `n` rows using the `take()` function, but the output is a list of Row objects, not formatted as a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f1cf3e0-245b-4cb7-844f-e3f7675d75d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(Name='Alice', Age=34), Row(Name='Bob', Age=45)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0a7321a-3b8c-467f-ab20-60cb0e750201",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's get the name of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5514be4-6a4f-410d-a425-42c54ba6f4cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Name', 'Age']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65c55c64-9ea3-43b2-985e-909e5307e014",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's get the number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d519e505-fa27-4967-ba96-2ba3df02aad8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c170c33-b237-4eb6-a09a-3136bfa8f54c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Data types and schema in Spark DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "571f83ae-ee5c-4430-9ad9-1ce99147afa5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The schema of a Spark DataFrame is the combination of column names and the data types associated with each of these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "594ca473-5d3d-4755-aeb1-199733b81efe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Name: string (nullable = true)\n |-- Age: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ade32af8-e269-453f-9d6d-e210e14f4144",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "When Spark creates a new DataFrame, it will automatically guess which schema is appropriate for that DataFrame. In other words, Spark will try to guess which are the appropriate data types for each column. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31aad618-4991-403d-abb6-e8b605ad0ddb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You can create a dataframe with a predefined schema. For example, we want to set Age as integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94f27dd3-7f53-40d6-8b28-37600cd00d73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Name: string (nullable = true)\n |-- Age: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql import Row\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "data = [Row(Name=\"Alice\", Age=30), Row(Name=\"Bob\", Age=25), Row(Name=\"Charlie\", Age=35)]\n",
    "\n",
    "sample_df = spark.createDataFrame(data, schema)\n",
    "sample_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8f3e05a-6142-49a6-8c52-187ca2aa5a00",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Besides the \"standard\" data types such as Integer, Float, Double, String, etc..., Spark DataFrame also support two more complex types which are `ArrayType` and `MapType`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df8979ca-3429-4f10-9e2d-941183d5e337",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "`ArrayType` represents a column that contains an array of elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7ce6b2e-c61b-4fcb-b395-cbeb42330465",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n| Name|             Hobbies|\n+-----+--------------------+\n|Alice|   [Reading, Hiking]|\n|  Bob| [Cooking, Swimming]|\n|Cathy|[Traveling, Dancing]|\n+-----+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "\n",
    "# Define schema with ArrayType\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Hobbies\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"Alice\", [\"Reading\", \"Hiking\"]),\n",
    "    (\"Bob\", [\"Cooking\", \"Swimming\"]),\n",
    "    (\"Cathy\", [\"Traveling\", \"Dancing\"])\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2781f4f0-86a5-45ba-a962-e9f051299233",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "`MapType` represents a column that contains a map of key-value pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc1dd12d-d4c1-4c31-84cc-f7e0e423b385",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n| Name|          Attributes|\n+-----+--------------------+\n|Alice|{Height -> 5.5, W...|\n|  Bob|{Height -> 6.0, W...|\n|Cathy|{Height -> 5.7, W...|\n+-----+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, MapType\n",
    "\n",
    "# Define schema with MapType\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Attributes\", MapType(StringType(), StringType()), True)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"Alice\", {\"Height\": \"5.5\", \"Weight\": \"130\"}),\n",
    "    (\"Bob\", {\"Height\": \"6.0\", \"Weight\": \"180\"}),\n",
    "    (\"Cathy\", {\"Height\": \"5.7\", \"Weight\": \"150\"})\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd60cb3c-c4ec-4e3a-a32a-dddbb474a5e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Transformations of Spark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d44a8237-5366-4b97-803d-cb42221995e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "List of Transformations in Spark DataFrame\n",
    "\n",
    "- `select()`\n",
    "- `filter()`\n",
    "- `groupBy()`\n",
    "- `agg()`\n",
    "- `join()`\n",
    "- `withColumn()`\n",
    "- `drop()`\n",
    "- `distinct()`\n",
    "- `orderBy()`\n",
    "- `limit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9fa9fde-4f8a-443b-9708-293fce1f4c18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "702a4c1a-fd07-4b1d-b8fb-4407839d24dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n| Name|\n+-----+\n|Alice|\n|  Bob|\n|Cathy|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09affd14-fd05-4440-b91f-93ab562d81e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n| Name|Age|\n+-----+---+\n|Alice| 34|\n+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.Name == 'Alice').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "498890ae-ba40-43d1-9bae-4e2b1fb05333",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n| Name|Age|\n+-----+---+\n|Cathy| 29|\n+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.filter(col(\"Age\") < 30).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b906ebf-24d9-4cc5-9898-106934a336a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[Name: string]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df\\\n",
    "    .filter(df.Age > 30)\\\n",
    "    .select(\"Name\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1906ea1-8176-48f0-a81a-da5702e9e2da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Each one of these DataFrame methods create a **lazily evaluated** transformation. Spark will only check if they make sense with the initial DataFrame that you have. Spark will not actually perform these transformations on your initial DataFrame, not untill you trigger these **transformations** with an **action**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "066cf988-f851-4ce4-be32-2199654937ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n| Name|\n+-----+\n|Alice|\n|  Bob|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0b82287-484c-4d2c-933f-bde26f23bddd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "you can use a multi-line string to define a condition for filtering a DataFrame in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24122ac5-aa27-46f9-ada6-96902ae8abf5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n| Name|Age|\n+-----+---+\n|Alice| 34|\n|Cathy| 29|\n+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "condition = '''\n",
    "    Age < 30\n",
    "    or Name = 'Alice'\n",
    "'''\n",
    "df.filter(condition).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3abd032e-230f-4088-8fa1-0ab3269caca8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Actions in Spark DataFrame\n",
    "\n",
    "Here are some common actions in Spark DataFrame, grouped by similarity:\n",
    "\n",
    "- **Show and Display**\n",
    "  - `show()`\n",
    "  - `head()`\n",
    "  - `first()`\n",
    "  - `take()`\n",
    "\n",
    "- **Aggregation and Statistics**\n",
    "  - `count()`\n",
    "  - `describe()`\n",
    "  - `summary()`\n",
    "  - `agg()`\n",
    "\n",
    "- **Collection and Conversion**\n",
    "  - `collect()`\n",
    "  - `toPandas()`\n",
    "  - `toJSON()`\n",
    "\n",
    "- **Saving and Writing**\n",
    "  - `write()`\n",
    "  - `save()`\n",
    "  - `saveAsTable()`"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "lecture11",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
