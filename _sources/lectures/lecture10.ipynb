{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d894202-16a1-4ad4-a057-b2f05282c3e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Lecture 10: Resilient Distributed Datasets (RDDs) in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b62e6e7-43a2-46aa-a9f3-123485859825",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this lecture, students should be able to:\n",
    "\n",
    "- Understand the concept of Resilient Distributed Datasets (RDDs) in Apache Spark.\n",
    "- Explain the immutability and fault tolerance features of RDDs.\n",
    "- Describe how RDDs enable parallel processing in a distributed computing environment.\n",
    "- Identify the key features of RDDs, including lazy evaluation and partitioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3db31e7e-0c57-45fb-9a79-2b3cbe53a85d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Introduction to RDDs\n",
    "\n",
    "- **Resilient Distributed Datasets (RDDs)** are a foundational component in Apache Spark. They represent a distributed collection of data, partitioned across a cluster, which allows for parallel processing. RDDs are the core abstraction that enables Spark to handle large datasets efficiently across multiple machines.\n",
    "\n",
    "- **Immutability**: Once created, RDDs cannot be modified. Instead of changing an existing RDD, transformations (like map, filter, etc.) generate a new RDD from the original one. This immutability helps ensure consistency across distributed systems, as different machines or processes are always working with the same, unchangeable data.\n",
    "\n",
    "- **Parallel processing**: Since RDDs are distributed across multiple nodes, operations on them can be performed in parallel, leveraging the full power of a distributed computing environment. This makes it possible to process vast amounts of data much faster than on a single machine.\n",
    "\n",
    "- **Fault tolerance**: One of the key features of RDDs is their ability to recover from failures without the need to replicate the entire dataset. This is achieved through lineage, a concept where RDDs keep track of how they were derived from other RDDs. If a node or partition fails, Spark can use this lineage information to recompute only the affected parts of the dataset, minimizing data loss and avoiding the overhead of redundant copies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aba04f99-8541-4e03-a138-ffb10f5f8949",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Key Features of RDDs\n",
    "### 1. **Immutability**\n",
    "Once created, RDDs cannot be changed. Transformations on RDDs produce new RDDs.\n",
    "\n",
    "What is immutable object? \n",
    "\n",
    "In Python, immutability refers to objects whose state cannot be modified after they are created. For example, `string`, `number`, `tuples` are immutable objects in Python.\n",
    "\n",
    "For example, when you try to overwrite the first letter of a string, it will give an error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a10cd8a-7c28-4e60-afa3-16d524816377",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1503943367388992>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m string1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHello world\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
       "\u001B[0;32m----> 2\u001B[0m string1[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mA\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: 'str' object does not support item assignment"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "TypeError",
        "evalue": "'str' object does not support item assignment"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: 'str' object does not support item assignment"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-1503943367388992>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m string1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHello world\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m----> 2\u001B[0m string1[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mA\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
        "\u001B[0;31mTypeError\u001B[0m: 'str' object does not support item assignment"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "string1 = 'Hello world'\n",
    "string1[0] = 'A'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f15cc375-a27a-4ebe-a777-42a20349fd12",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Same for tuple. You cannot overwrite the object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a99e49e-f419-4020-afe6-6a920cea9784",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1503943367389002>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m tuple1 \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m)\n",
       "\u001B[0;32m----> 2\u001B[0m tuple1[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m4\u001B[39m\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: 'tuple' object does not support item assignment"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "TypeError",
        "evalue": "'tuple' object does not support item assignment"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: 'tuple' object does not support item assignment"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-1503943367389002>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m tuple1 \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m tuple1[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m4\u001B[39m\n",
        "\u001B[0;31mTypeError\u001B[0m: 'tuple' object does not support item assignment"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tuple1 = (1, 2, 3)\n",
    "tuple1[0] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19621c42-ddfe-4f73-affc-91cbd548ccec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "But with list you can, because list is mutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22f789a4-bcf3-46a6-8d03-78c48beb8b74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[4, 2, 3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 = [1, 2, 3]\n",
    "list1[0] = 4\n",
    "list1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fc3bb04-133c-4573-a391-9aacbcc0df63",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now come back to RDDs, what immutability means here is the fact that any transformation done on the data will create a new copy of the data, instead of overwriting it. This ensures that the original dataset remains unchanged, allowing for safer parallel processing and easier debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f26d6859-801c-476c-8e1b-317c2f803067",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 2. **Lazy Evaluation**\n",
    "Transformations on RDDs are not executed immediately. They are computed only when an `action` is called. This means that Spark builds up a plan of transformations to apply when an action is finally invoked, optimizing the execution process and reducing unnecessary computations.\n",
    "\n",
    "For example let's take the following dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cfae656-7fcd-4ca9-b59d-2bf448db765c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th></tr></thead><tbody><tr><td>1</td><td>Alice</td></tr><tr><td>2</td><td>Bob</td></tr><tr><td>3</td><td>Cathy</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Alice"
        ],
        [
         2,
         "Bob"
        ],
        [
         3,
         "Cathy"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a Spark DataFrame\n",
    "data = [(1, 'Alice'), (2, 'Bob'), (3, 'Cathy')]\n",
    "columns = ['id', 'name']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccdc82d8-eb0d-431b-9f11-367e2d87dded",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We want to filter the dataset based on `id` column. When you run the code cell below, actually nothing happen. PySpark will record the transformation but do not execute it yet. Think of it as PySpark writes down a recipe, but no food has been cooked yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2fcef5d-9aba-434b-bef4-a9b65ce7e174",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a transformation (lazy evaluation)\n",
    "filtered_df = df.filter(df['id'] > 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f82608c-5c18-4fbe-9f5b-9c1d2a130099",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It will only execute the `transformations` when we trigger an `action`, such as `display`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdf93d7d-a468-4e29-be1d-2d282da08c9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th></tr></thead><tbody><tr><td>2</td><td>Bob</td></tr><tr><td>3</td><td>Cathy</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2,
         "Bob"
        ],
        [
         3,
         "Cathy"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Action to trigger the evaluation\n",
    "display(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "807b4a68-35dc-4ac3-acf1-f938de0d9ece",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3. **Fault Tolerance**\n",
    "RDDs track lineage information to rebuild lost data. For example, if a partition of an RDD is lost due to a node failure, the RDD can use its lineage information to recompute the lost partition from the original data source or from other RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "473c9a99-9c55-4c43-8e2c-1e9cf42edc53",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 4. **Partitioning**\n",
    "\n",
    "RDDs are divided into partitions, which can be processed in parallel across the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db4ea32b-017b-4d15-bfa7-4b58aa521f3b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:583/1*rtyvHjINsHF_5yHf8c0Z0Q.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "183a4b9e-48a8-488a-a335-0be9929c6f57",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Transformations & Actions\n",
    "\n",
    "As we discussed above, RDDs is lazy-evaluated, which means it won't execute the `transformations` until an `action` is triggered. Let's have a closer look at this using some examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef20fbcd-da6d-4785-bf6f-66cae27851f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "#### Example: Creating RDDs from a csv File\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66b32762-a372-4b46-8caf-25b1066069e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "First, let's try to read in a data file from an URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d280aa15-511b-468f-9fd5-e893e6cfc138",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import CSV from the given URL\n",
    "\n",
    "url = \"https://github.com/selva86/datasets/raw/master/AirPassengers.csv\"\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "# Add the file to the SparkContext\n",
    "spark.sparkContext.addFile(url)\n",
    "\n",
    "# Read the file\n",
    "rdd = spark.sparkContext.textFile(\"file://\"+SparkFiles.get(\"AirPassengers.csv\"))\n",
    "\n",
    "# Extract header\n",
    "header = rdd.first()\n",
    "\n",
    "# Remove header and split by commas\n",
    "rdd = rdd.filter(lambda row: row != header).map(lambda row: row.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "369e0eda-74ce-4b85-88f5-b480cb0a123e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Can you guess where the data are being stored? They were actually partitioned and stored acrossed multiple servers. This is distributed computing! To see the number of partitions, we can run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10e4df1b-76c1-493f-bd2f-c9ae9372d67a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddeebe55-f311-460b-af33-6ebaa91c41fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's see what happens when we just try to display the dataframe by typing df. Typically, if this was a pandas.DataFrame, it woul display the first few rows. \n",
    "\n",
    "However, since Spark is lazy-evaluated, it doesn't display the data yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d91e254-9a4c-45bf-84a2-8208590e769a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PythonRDD[58] at RDD at PythonRDD.scala:61"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4eb8ceb-8fd6-4bca-b8a1-2cedeb2457b2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To display the data, you need to trigger an `action`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4fd530e-9fab-4843-b2de-d97842d6cdf5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now let's trigger an action called `collect()`. The `collect()` function in PySpark is used to retrieve the entire dataset from the distributed environment (i.e., the cluster) back to the driver program as a list. \n",
    "\n",
    "This method is often used for debugging or small datasets because it brings all the data into the driver's memory, which can lead to memory issues if the dataset is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa9d37db-cdd1-4f41-8cdb-be655e65d0cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['1949-01-01', '112'],\n",
       " ['1949-02-01', '118'],\n",
       " ['1949-03-01', '132'],\n",
       " ['1949-04-01', '129'],\n",
       " ['1949-05-01', '121'],\n",
       " ['1949-06-01', '135'],\n",
       " ['1949-07-01', '148'],\n",
       " ['1949-08-01', '148'],\n",
       " ['1949-09-01', '136'],\n",
       " ['1949-10-01', '119'],\n",
       " ['1949-11-01', '104'],\n",
       " ['1949-12-01', '118'],\n",
       " ['1950-01-01', '115'],\n",
       " ['1950-02-01', '126'],\n",
       " ['1950-03-01', '141'],\n",
       " ['1950-04-01', '135'],\n",
       " ['1950-05-01', '125'],\n",
       " ['1950-06-01', '149'],\n",
       " ['1950-07-01', '170'],\n",
       " ['1950-08-01', '170'],\n",
       " ['1950-09-01', '158'],\n",
       " ['1950-10-01', '133'],\n",
       " ['1950-11-01', '114'],\n",
       " ['1950-12-01', '140'],\n",
       " ['1951-01-01', '145'],\n",
       " ['1951-02-01', '150'],\n",
       " ['1951-03-01', '178'],\n",
       " ['1951-04-01', '163'],\n",
       " ['1951-05-01', '172'],\n",
       " ['1951-06-01', '178'],\n",
       " ['1951-07-01', '199'],\n",
       " ['1951-08-01', '199'],\n",
       " ['1951-09-01', '184'],\n",
       " ['1951-10-01', '162'],\n",
       " ['1951-11-01', '146'],\n",
       " ['1951-12-01', '166'],\n",
       " ['1952-01-01', '171'],\n",
       " ['1952-02-01', '180'],\n",
       " ['1952-03-01', '193'],\n",
       " ['1952-04-01', '181'],\n",
       " ['1952-05-01', '183'],\n",
       " ['1952-06-01', '218'],\n",
       " ['1952-07-01', '230'],\n",
       " ['1952-08-01', '242'],\n",
       " ['1952-09-01', '209'],\n",
       " ['1952-10-01', '191'],\n",
       " ['1952-11-01', '172'],\n",
       " ['1952-12-01', '194'],\n",
       " ['1953-01-01', '196'],\n",
       " ['1953-02-01', '196'],\n",
       " ['1953-03-01', '236'],\n",
       " ['1953-04-01', '235'],\n",
       " ['1953-05-01', '229'],\n",
       " ['1953-06-01', '243'],\n",
       " ['1953-07-01', '264'],\n",
       " ['1953-08-01', '272'],\n",
       " ['1953-09-01', '237'],\n",
       " ['1953-10-01', '211'],\n",
       " ['1953-11-01', '180'],\n",
       " ['1953-12-01', '201'],\n",
       " ['1954-01-01', '204'],\n",
       " ['1954-02-01', '188'],\n",
       " ['1954-03-01', '235'],\n",
       " ['1954-04-01', '227'],\n",
       " ['1954-05-01', '234'],\n",
       " ['1954-06-01', '264'],\n",
       " ['1954-07-01', '302'],\n",
       " ['1954-08-01', '293'],\n",
       " ['1954-09-01', '259'],\n",
       " ['1954-10-01', '229'],\n",
       " ['1954-11-01', '203'],\n",
       " ['1954-12-01', '229'],\n",
       " ['1955-01-01', '242'],\n",
       " ['1955-02-01', '233'],\n",
       " ['1955-03-01', '267'],\n",
       " ['1955-04-01', '269'],\n",
       " ['1955-05-01', '270'],\n",
       " ['1955-06-01', '315'],\n",
       " ['1955-07-01', '364'],\n",
       " ['1955-08-01', '347'],\n",
       " ['1955-09-01', '312'],\n",
       " ['1955-10-01', '274'],\n",
       " ['1955-11-01', '237'],\n",
       " ['1955-12-01', '278'],\n",
       " ['1956-01-01', '284'],\n",
       " ['1956-02-01', '277'],\n",
       " ['1956-03-01', '317'],\n",
       " ['1956-04-01', '313'],\n",
       " ['1956-05-01', '318'],\n",
       " ['1956-06-01', '374'],\n",
       " ['1956-07-01', '413'],\n",
       " ['1956-08-01', '405'],\n",
       " ['1956-09-01', '355'],\n",
       " ['1956-10-01', '306'],\n",
       " ['1956-11-01', '271'],\n",
       " ['1956-12-01', '306'],\n",
       " ['1957-01-01', '315'],\n",
       " ['1957-02-01', '301'],\n",
       " ['1957-03-01', '356'],\n",
       " ['1957-04-01', '348'],\n",
       " ['1957-05-01', '355'],\n",
       " ['1957-06-01', '422'],\n",
       " ['1957-07-01', '465'],\n",
       " ['1957-08-01', '467'],\n",
       " ['1957-09-01', '404'],\n",
       " ['1957-10-01', '347'],\n",
       " ['1957-11-01', '305'],\n",
       " ['1957-12-01', '336'],\n",
       " ['1958-01-01', '340'],\n",
       " ['1958-02-01', '318'],\n",
       " ['1958-03-01', '362'],\n",
       " ['1958-04-01', '348'],\n",
       " ['1958-05-01', '363'],\n",
       " ['1958-06-01', '435'],\n",
       " ['1958-07-01', '491'],\n",
       " ['1958-08-01', '505'],\n",
       " ['1958-09-01', '404'],\n",
       " ['1958-10-01', '359'],\n",
       " ['1958-11-01', '310'],\n",
       " ['1958-12-01', '337'],\n",
       " ['1959-01-01', '360'],\n",
       " ['1959-02-01', '342'],\n",
       " ['1959-03-01', '406'],\n",
       " ['1959-04-01', '396'],\n",
       " ['1959-05-01', '420'],\n",
       " ['1959-06-01', '472'],\n",
       " ['1959-07-01', '548'],\n",
       " ['1959-08-01', '559'],\n",
       " ['1959-09-01', '463'],\n",
       " ['1959-10-01', '407'],\n",
       " ['1959-11-01', '362'],\n",
       " ['1959-12-01', '405'],\n",
       " ['1960-01-01', '417'],\n",
       " ['1960-02-01', '391'],\n",
       " ['1960-03-01', '419'],\n",
       " ['1960-04-01', '461'],\n",
       " ['1960-05-01', '472'],\n",
       " ['1960-06-01', '535'],\n",
       " ['1960-07-01', '622'],\n",
       " ['1960-08-01', '606'],\n",
       " ['1960-09-01', '508'],\n",
       " ['1960-10-01', '461'],\n",
       " ['1960-11-01', '390'],\n",
       " ['1960-12-01', '432']]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43e87b42-0a60-4b03-afd3-83fba43714bb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The `take()` function in PySpark is used to retrieve a specified number of elements from an RDD or DataFrame. It returns the first n elements as a list, where n is the number you specify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f4241c2-4e44-48dc-a490-4af78bc70c30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['1949-01-01', '112'],\n",
       " ['1949-02-01', '118'],\n",
       " ['1949-03-01', '132'],\n",
       " ['1949-04-01', '129'],\n",
       " ['1949-05-01', '121'],\n",
       " ['1949-06-01', '135'],\n",
       " ['1949-07-01', '148'],\n",
       " ['1949-08-01', '148'],\n",
       " ['1949-09-01', '136'],\n",
       " ['1949-10-01', '119']]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09ba8b02-aced-4467-99d1-e5bd2103fd48",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "#### RDD Transformations\n",
    "- Transformations are operations on RDDs that return a new RDD.\n",
    "- Examples include `map()`, `filter()`, `flatMap()`, `reduceByKey()`, and `join()`.\n",
    "\n",
    "##### Example: Using `map()` and `filter()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "437d2a15-0059-416d-94e8-0cc087caf19d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PythonRDD[67] at RDD at PythonRDD.scala:61"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an RDD from a list\n",
    "rdd = sc.parallelize(range(1, 101))\n",
    "\n",
    "# Use map to square each element\n",
    "squared_rdd = rdd.map(lambda x: x ** 2)\n",
    "\n",
    "# Apply a filter transformation to keep only even numbers\n",
    "even_rdd = squared_rdd.filter(lambda x: x % 2 == 0)\n",
    "\n",
    "squared_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64540c17-ebd8-4084-acf3-37ff4f9f7228",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Notice that when we try to print out squared_rdd, nothing happens, because we have not trigger an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32531f0d-4576-45ae-b113-d9e169638f81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 16, 36, 64, 100, 144, 196, 256, 324, 400, 484, 576, 676, 784, 900, 1024, 1156, 1296, 1444, 1600, 1764, 1936, 2116, 2304, 2500, 2704, 2916, 3136, 3364, 3600, 3844, 4096, 4356, 4624, 4900, 5184, 5476, 5776, 6084, 6400, 6724, 7056, 7396, 7744, 8100, 8464, 8836, 9216, 9604, 10000]\n"
     ]
    }
   ],
   "source": [
    "# Collect and print the results\n",
    "print(even_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "395907ff-c082-4532-b595-d2bda61a5417",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "A better way would be using the `take()` function, since it won't bring the whole dataset into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f8c58a2-9bb1-47b9-8007-434251372827",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[4, 16, 36]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66858b64-cd6b-49ef-853a-8672ab43d9a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Example of `join`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36690750-491f-4931-9b10-c65190f0fcf5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, ('apple', 'red')), (2, ('banana', 'yellow')), (3, ('cherry', 'red'))]\n"
     ]
    }
   ],
   "source": [
    "# Create two RDDs with key-value pairs\n",
    "rdd1 = sc.parallelize([(1, \"apple\"), (2, \"banana\"), (3, \"cherry\")])\n",
    "rdd2 = sc.parallelize([(1, \"red\"), (2, \"yellow\"), (3, \"red\"), (4, \"green\")])\n",
    "\n",
    "# Perform a join operation on the two RDDs\n",
    "joined_rdd = rdd1.join(rdd2)\n",
    "\n",
    "# Collect and print the results\n",
    "print(joined_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73c0225c-712b-46ec-a6ab-c982519ad446",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "#### RDD Actions\n",
    "- Actions are operations that trigger the execution of transformations and return a result.\n",
    "- Examples include `collect()`, `count()`, `take()`, and `reduce()`.\n",
    "\n",
    "##### Example: Using `reduce()`\n",
    "\n",
    "The reduce() function in PySpark is an action that aggregates the elements of an RDD using a specified binary operator. It takes a function that operates on two elements of the RDD and returns a single element.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca2b9e63-48e2-426e-851b-81dc77da2c07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5050\n"
     ]
    }
   ],
   "source": [
    "# Use reduce to sum all elements\n",
    "sum_result = rdd.reduce(lambda x, y: x + y)\n",
    "\n",
    "# Print the result\n",
    "print(sum_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "800c6a2b-f04a-4875-99cf-779447234f98",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 5 Reasons on When to use RDDs\n",
    "\n",
    "- You want low-level transformation and actions and control on your dataset;\n",
    "- Your data is unstructured, such as media streams or streams of text;\n",
    "- You want to manipulate your data with functional programming constructs than domain specific expressions;\n",
    "- You donâ€™t care about imposing a schema, such as columnar format while processing or accessing data attributes by name or column; and\n",
    "- You can forgo some optimization and performance benefits available with DataFrames and Datasets for structured and semi-structured data.\n",
    "\n",
    "References\n",
    "- [Apache Spark Documentation](https://www.databricks.com/glossary/what-is-rdd)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "lecture10",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
